{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Tree Classification Template\n",
    "\n",
    "## Notebook for Decision Tree/Random Forest/Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, we will be tackling the churn prediction problem on a very unique and interesting group of subscribers on a video streaming service! \n",
    "\n",
    "Imagine that you are a new data scientist at this video streaming company and you are tasked with building a model that can predict which existing subscribers will continue their subscriptions for another month. We have provided a dataset that is a sample of subscriptions that were initiated in 2021, all snapshotted at a particular date before the subscription was cancelled. Subscription cancellation can happen for a multitude of reasons, including:\n",
    "* the customer completes all content they were interested in, and no longer need the subscription\n",
    "* the customer finds themselves to be too busy and cancels their subscription until a later time\n",
    "* the customer determines that the streaming service is not the best fit for them, so they cancel and look for something better suited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Tasks\n",
    "\n",
    "### 1) Understand the shape of the data (Histograms, box plots, etc.)\n",
    "\n",
    "### 2) Data Cleaning \n",
    "\n",
    "### 3) Data Exploration\n",
    "\n",
    "### 4) Feature Engineering \n",
    "\n",
    "### 5) Data Preprocessing for Model\n",
    "\n",
    "### 6) Basic Model Building \n",
    "\n",
    "### 7) Model Tuning \n",
    "\n",
    "### 8) Ensemble Model Building \n",
    "\n",
    "### 9) Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from numpy import count_nonzero, median, mean\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler, Binarizer, OrdinalEncoder\n",
    "\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score\n",
    "\n",
    "# from sklearn.feature_selection import f_classif, chi2, RFE, RFECV\n",
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "# from sklearn.feature_selection import VarianceThreshold, GenericUnivariateSelect\n",
    "# from sklearn.feature_selection import SelectFromModel, SelectKBest, SelectPercentile\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay \n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import feature_engine\n",
    "\n",
    "from feature_engine.selection import (DropConstantFeatures, DropDuplicateFeatures, \n",
    "                                      DropCorrelatedFeatures, SmartCorrelatedSelection)\n",
    "from feature_engine.selection import SelectBySingleFeaturePerformance, SelectByShuffling, RecursiveFeatureElimination\n",
    "from feature_engine.selection import RecursiveFeatureAddition\n",
    "\n",
    "%matplotlib inline\n",
    "#sets the default autosave frequency in seconds\n",
    "%autosave 60 \n",
    "sns.set_style('dark')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "\n",
    "font = {'family' : 'monospace',\n",
    "          'weight' : 'bold',\n",
    "          'size'   : '20'}\n",
    "plt.rc('font' , **font)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import pickle\n",
    "# from pickle import dump, load\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "#pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format','{:.2f}'.format)\n",
    "\n",
    "# Ensure results are reproducible\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_descriptions = pd.read_csv('data_descriptions.csv')\n",
    "data_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quick Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"dftrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistical Analysis\n",
    "df_train.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistical Analysis\n",
    "df_train.describe(include=[\"int\", \"float\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistical Analysis\n",
    "df_train.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "df_train['churn'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['churn'].value_counts(normalize=True).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"dftestnolabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessor.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testdata = preprocessor.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==================================================================================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.heatmap(df_train.corr(),cmap=\"coolwarm\",annot=True,fmt='.2f',linewidths=2)\n",
    "plt.title(\"Correlation Heatmap\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a small dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sample(frac=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.iloc[:,0:19]\n",
    "y = df_train.iloc[:,19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train),Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter[0])\n",
    "print(counter[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate scale_pos_weight value\n",
    "estimate = counter[0] / counter[1]\n",
    "estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = 4.54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipelines\n",
    "\n",
    "Data Pipelines simplify the steps of processing the data. We use the module <code>Pipeline</code> to create a pipeline. \n",
    "`Pipeline` lets you chain together multiple operators on your data that both have a `fit` method.\n",
    "\n",
    "### Combine multiple processing steps into a `Pipeline`\n",
    "\n",
    "A pipeline contains a series of steps, where a step is (\"name of step\", actual_model). The \"name of step\" string is only used to help you identify which step you are on, and to allow you to specify parameters at that step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare preprocessing functions\n",
    "\n",
    "#imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "#ohe = OneHotEncoder()\n",
    "#oe = OrdinalEncoder()\n",
    "#ss = StandardScaler()\n",
    "#mm = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_train.select_dtypes(include=[\"int64\",\"float64\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_train.select_dtypes(include=[\"bool\",\"object\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numcols = ['accountage', 'monthlycharges', 'totalcharges', 'viewinghoursperweek', 'averageviewingduration',\n",
    " 'contentdownloadspermonth', 'userrating', 'supportticketspermonth', 'watchlistsize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols = ['subscriptiontype', 'paymentmethod', 'paperlessbilling', 'contenttype', 'multideviceaccess', 'deviceregistered',\n",
    " 'genrepreference', 'gender', 'parentalcontrol', 'subtitlesenabled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the preprocessing pipelines for both\n",
    "# numerical and categorical data\n",
    "\n",
    "\n",
    "drop_transformer = ColumnTransformer(transformers=\n",
    "                                    (\"dropcolumns\", \"drop\", dropcols)\n",
    "                                    )\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "                             # (\"scalar\", StandardScaler()),\n",
    "                              (\"minmax\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "                                  (\"onehot\", OneHotEncoder(sparse_output=False, drop='if_binary')),\n",
    "    \n",
    "                                  #(\"ordinal\", OrdinalEncoder(categories='auto', handle_unknown=\"error\"))\n",
    "   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "               transformers=[\n",
    "                           (\"dropcolumns\", \"drop\", dropcols),\n",
    "                           (\"numerical\", numeric_transformer, numcols),\n",
    "                           (\"categorical\", categorical_transformer, catcols),\n",
    "                   \n",
    "                            ],\n",
    "               remainder=\"passthrough\",\n",
    "               verbose_feature_names_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "               transformers=[\n",
    "                           (\"dropcolumns\", \"drop\", [\"id\"]),\n",
    "                           (\"numerical\", numeric_transformer, numcols),\n",
    "                           #(\"categorical\", categorical_transformer, catcols),\n",
    "                   \n",
    "                            ],\n",
    "               remainder=\"drop\",\n",
    "               verbose_feature_names_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check features transformation (Train Set)\n",
    "\n",
    "preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check features transformation (Test Set)\n",
    "\n",
    "preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocessor.fit_transform(X_train)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model (Baseline)\n",
    "\n",
    "The `DecisionTreeClassifier` has many arguments (model hyperparameters) that can be customized and eventually tune the generated decision tree classifiers. Among these arguments, there are three commonly tuned arguments as follows:\n",
    "- criterion: `gini` or `entropy`, which specifies which criteria to be used when splitting a tree node\n",
    "- max_depth: a numeric value to specify the max depth of the tree. Larger tree depth normally means larger model complexity\n",
    "- min_samples_leaf: The minimal number of samples in leaf nodes. Larger samples in leaf nodes will tend to generate simpler trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtpipeline = Pipeline(steps=[\n",
    "                        (\"preprocessor\", preprocessor),\n",
    "                        (\"decisiontree\", DecisionTreeClassifier(random_state=0))\n",
    "                    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtpipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtpred = dtpipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtpred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the second column of probabilities (class 1) and rename it\n",
    "dt_predicted_probability = dtpred[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predicted_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree Classifier\\n\")\n",
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, dtpred))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, dtpred))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, dtpred))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, dtpred))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, dtpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree Classifier\\n\")\n",
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, dt_predicted_probability))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, dt_predicted_probability))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, dt_predicted_probability))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, dt_predicted_probability))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, dt_predicted_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, dtpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcm = confusion_matrix(y_test, dtpred)\n",
    "dtcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=dtpipeline, X=X_test, y=y_test, ax=ax, \n",
    "                                      labels=dtpipeline.classes_, cmap=\"viridis\")\n",
    "\n",
    "ax.set_title('Confusion matrix of the classifier', size=20)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=dtpipeline, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=20)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "PrecisionRecallDisplay.from_estimator(estimator=dtpipeline, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('Precision/Recall of the classifier', size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Validation (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcv = cross_validate(estimator=dtpipeline, X=X_train, y=y_train, scoring=\"roc_auc\", cv=skf, n_jobs=2, return_train_score=True)\n",
    "dtcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcv[\"train_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcv[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = cross_val_score(estimator=dtpipeline, X=X_train, y=y_train, scoring=\"roc_auc\", cv=skf, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtpipeline.named_steps.decisiontree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "plot_tree(dtpipeline.named_steps.decisiontree, max_depth=2, feature_names=X.columns, class_names=['0','1'], fontsize=14, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessor.fit_transform(X_train).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = dtpipeline.named_steps.decisiontree.feature_importances_\n",
    "\n",
    "feature_importances = pd.Series(importances, index=preprocessor.fit_transform(X_train).columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "feature_importances.plot.barh(ax=ax, figsize=(12,8))\n",
    "ax.set_title(\"Decision Tree Feature Importances\")\n",
    "ax.tick_params('x', rotation=0)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = dtpipeline.named_steps.decisiontree.feature_importances_\n",
    "\n",
    "# feature_importances = pd.Series(importances, index=preprocessor.fit_transform(X_train).columns)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# feature_importances.plot.barh(ax=ax, figsize=(12,8))\n",
    "# ax.set_title(\"Decision Tree Feature Importances\")\n",
    "# ax.tick_params('x', rotation=0)\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "#fig.savefig(\"tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_df = pd.DataFrame(feature_importances, columns=[\"importances\"])\n",
    "feature_importances_df = feature_importances_df.sort_values(by='importances')\n",
    "feature_importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "sns.barplot(data=feature_importances_df, x=feature_importances_df.importances, y=feature_importances_df.index, orient='h')\n",
    "\n",
    "ax.set_title(\"Decision Tree: Feature Importances\", fontsize=20)\n",
    "\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipeline = Pipeline(steps=[\n",
    "                        (\"preprocessor\", preprocessor),\n",
    "                        (\"randomforest\", RandomForestClassifier(random_state=0))\n",
    "                    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpred = rfpipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Tree Classifier\\n\")\n",
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, rfpred))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, rfpred))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, rfpred))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, rfpred))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, rfpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcm = confusion_matrix(y_test,rfpred)\n",
    "rfcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,rfpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=rfpipeline, X=X_test, y=y_test, ax=ax, display_labels=rfpipeline.classes_)\n",
    "ax.set_title('Confusion matrix of the classifier', size=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=rfpipeline, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Validation (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcv = cross_validate(estimator=rfpipeline, X=X_train, y=y_train, scoring=\"roc_auc\", cv=skf, n_jobs=2, return_train_score=False)\n",
    "rfcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcv[\"train_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcv[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcv2 = cross_val_score(estimator=rfpipeline, X=X_train, y=y_train, scoring=\"roc_auc\", cv=skf, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipeline.named_steps.randomforest.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'randomforest__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "              'randomforest__ccp_alpha': [0.0, 0.01, 0.02, 0.03, 0.04]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfgs = GridSearchCV(estimator=rfpipeline, param_grid=parameters, scoring=scoring, n_jobs=-1, cv=5, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rfgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfgs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearchCV (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'randomforest__n_estimators': stats.randint(50, 200),\n",
    "              'randomforest__max_depth' : stats.randint(2,10),\n",
    "              'randomforest__min_samples_split': stats.randint(2,5),\n",
    "              'randomforest__min_samples_leaf' : stats.randint(1,5),\n",
    "              'randomforest__ccp_alpha': stats.uniform(0,0.05)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randm = RandomizedSearchCV(estimator=rfpipeline, param_distributions = parameters, cv = 5, n_iter = 10, \n",
    "                           n_jobs=2, scoring=\"roc_auc\", refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_randm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestpipeline = rf_randm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also find the data for all models evaluated\n",
    "\n",
    "results = pd.DataFrame(rf_randm.cv_results_)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can order the different models based on their performance\n",
    "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[['param_randomforest__max_depth', 'param_randomforest__min_samples_leaf', \n",
    "         'param_randomforest__min_samples_split', 'param_randomforest__n_estimators',\n",
    "         'mean_test_score', 'rank_test_score']].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object):\n",
    "    '''\n",
    "    Accepts as arguments a model name (your choice - string) and\n",
    "    a fit GridSearchCV model object.\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean F1 score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(mean f1 score)\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_score'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "#     f1 = best_estimator_results.mean_test_f1\n",
    "#     recall = best_estimator_results.mean_test_recall\n",
    "#     precision = best_estimator_results.mean_test_precision\n",
    "#     accuracy = best_estimator_results.mean_test_accuracy\n",
    "    rocauc = best_estimator_results.mean_test_score\n",
    "  \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "#                         'F1': f1,\n",
    "#                         'Recall': recall,\n",
    "#                         'Precision': precision,\n",
    "#                         'Accuracy': accuracy,\n",
    "                        'ROC-AUC' : rocauc  \n",
    "                        },\n",
    "                        ignore_index=True\n",
    "                       )\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function on our model\n",
    "rf_result_table = make_results(\"Random Forest RCV\", rf_randm)\n",
    "\n",
    "rf_result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance (or Gini) graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rfpipeline.named_steps.randomforest.feature_importances_\n",
    "\n",
    "feature_importances = pd.Series(importances, index=preprocessor.fit_transform(X_train).columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "feature_importances.sort_values(ascending=False).plot.barh(ax=ax, figsize=(12,8))\n",
    "\n",
    "ax.set_title(\"Random Forest Feature Importances\", size=20)\n",
    "ax.tick_params('x', rotation=0)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances_df = pd.DataFrame(feature_importances, columns=[\"importances\"])\n",
    "# feature_importances_df = feature_importances_df.sort_values(by='importances')\n",
    "# feature_importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# sns.barplot(data=feature_importances_df, x=feature_importances_df.importances, y=feature_importances_df.index, orient='h')\n",
    "\n",
    "# ax.set_title(\"Decision Tree: Feature Importances for Employee Leaving\", fontsize=15)\n",
    "\n",
    "# ax.set_xlabel(\"Importance\")\n",
    "# ax.set_ylabel(\"Feature\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance\n",
    "\n",
    "Permutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular. This is especially useful for non-linear or opaque estimators. The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature.\n",
    "\n",
    "`\n",
    "permutation_importance(estimator, X,  y, scoring=None, n_repeats=5,\n",
    "                                   n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0)\n",
    "`\n",
    "\n",
    "We need to pass the model and the validation set to the permutation_importance function.\n",
    "\n",
    "The n_repeats parameter specifies the number of times the feature values are shuffled. More repetitions will give more accurate results, but will take longer to compute.\n",
    "\n",
    "The random_state parameter is used to set the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = permutation_importance(estimator=rfpipeline, X=X_test, y=y_test, n_jobs=-1, \n",
    "                            scoring=\"roc_auc\", random_state=0, n_repeats=10)\n",
    "\n",
    "pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm2 = permutation_importance(estimator=rf_randm.best_estimator_, X=X_test, y=y_test, n_jobs=-1, \n",
    "                            scoring=\"roc_auc\", random_state=0, n_repeats=10)\n",
    "\n",
    "pm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = pm2.importances_mean.argsort()\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(sorted_idx)), pm2.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "plt.title('Random Forest Permutation Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbcpipeline = Pipeline(steps=[\n",
    "                        (\"preprocessor\", preprocessor),\n",
    "                        (\"graboost\", GradientBoostingClassifier(random_state=0))\n",
    "                    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbcpipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbcpred = gbcpipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbcpred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient Boost Tree Classifier\\n\")\n",
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, gbcpred))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, gbcpred))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, gbcpred))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, gbcpred))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, gbcpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Validation GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbccv = cross_validate(estimator=gbcpipeline, X=X_train, y=y_train, scoring=\"roc_auc\", cv=kf, n_jobs=2, \n",
    "                    return_train_score=False)\n",
    "gbccv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbccv[\"train_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbccv[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearchCV (GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbcpipeline.named_steps.graboost.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'graboost__learning_rate': stats.uniform(0,1),\n",
    "              'graboost__n_estimators': stats.randint(50,250),\n",
    "              'graboost__min_samples_split' : stats.uniform(0,1),\n",
    "              'graboost__min_samples_leaf' : stats.uniform(0,1),\n",
    "              'graboost__max_depth': stats.randint(2,10)\n",
    "              \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_randm = RandomizedSearchCV(estimator=gbcpipeline, param_distributions = parameters, cv = 5, n_iter = 10, \n",
    "                           n_jobs=-1, scoring='roc_auc', refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbc_randm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_randm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_randm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_randm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also find the data for all models evaluated\n",
    "\n",
    "results = pd.DataFrame(gbc_randm.cv_results_)\n",
    "\n",
    "results.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can order the different models based on their performance\n",
    "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[['param_graboost__learning_rate','param_graboost__max_depth', 'param_graboost__min_samples_leaf',\n",
    "         'param_graboost__min_samples_split','param_graboost__n_estimators']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object):\n",
    "    '''\n",
    "    Accepts as arguments a model name (your choice - string) and\n",
    "    a fit GridSearchCV model object.\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean F1 score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(mean f1 score)\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_score'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    #f1 = best_estimator_results.mean_test_f1\n",
    "    #recall = best_estimator_results.mean_test_recall\n",
    "    #precision = best_estimator_results.mean_test_precision\n",
    "    #accuracy = best_estimator_results.mean_test_accuracy\n",
    "    rocauc = best_estimator_results.mean_test_score\n",
    "  \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "#                         'F1': f1,\n",
    "#                         'Recall': recall,\n",
    "#                         'Precision': precision,\n",
    "#                         'Accuracy': accuracy,\n",
    "                        'ROC-AUC' : rocauc  \n",
    "                        },\n",
    "                        ignore_index=True\n",
    "                       )\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function on our model\n",
    "gbc_result_table = make_results(\"Gradient Boosting RCV\", gbc_randm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance\n",
    "\n",
    "Permutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular. This is especially useful for non-linear or opaque estimators. The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature.\n",
    "\n",
    "`\n",
    "permutation_importance(estimator, X,  y, scoring=None, n_repeats=5,\n",
    "                                   n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0)\n",
    "`\n",
    "\n",
    "We need to pass the model and the validation set to the permutation_importance function.\n",
    "\n",
    "The n_repeats parameter specifies the number of times the feature values are shuffled. More repetitions will give more accurate results, but will take longer to compute.\n",
    "\n",
    "The random_state parameter is used to set the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbcpipeline = Pipeline(steps=[\n",
    "                        (\"preprocessor\", preprocessor),\n",
    "                        (\"graboost\", HistGradientBoostingClassifier(scoring=\"roc_auc\", random_state=0))\n",
    "                    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HistGradientBoosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbcpipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_pred = hgbcpipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the second column of probabilities (class 1) and rename it\n",
    "hgbc_predicted_probability = hgbc_pred[:, 1]\n",
    "\n",
    "hgbcprob = hgbc_predicted_probability.round(0)\n",
    "\n",
    "hgbcprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HistGradientBoosting Classifier\\n\")\n",
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, hgbc_pred))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, hgbc_pred))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, hgbc_pred))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, hgbc_pred))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, hgbc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HistGradientBoosting Classifier\\n\")\n",
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, hgbcprob))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, hgbcprob))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, hgbcprob))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, hgbcprob))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, hgbcprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgcv = cross_validate(estimator=hgbcpipeline, X=X_train, y=y_train, scoring=\"roc_auc\", cv=skf, n_jobs=2)\n",
    "hgcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgcv[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hgbcpipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'graboost__l2_regularization' : stats.uniform(0,1),\n",
    "              'graboost__learning_rate': stats.uniform(0,1),\n",
    "              'graboost__max_iter' : stats.randint(20,100),\n",
    "              'graboost__max_bins': stats.randint(10,100),\n",
    "              'graboost__max_depth': stats.randint(2,10),\n",
    "              'graboost__max_leaf_nodes' : stats.randint(2,15),\n",
    "              'graboost__min_samples_leaf': stats.randint(1,20)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm = RandomizedSearchCV(estimator=hgbcpipeline, param_distributions = parameters, cv = 5, n_iter = 15, \n",
    "                           n_jobs=2, scoring='roc_auc', refit=True, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hgbc_randm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also find the data for all models evaluated\n",
    "\n",
    "results = pd.DataFrame(hgbc_randm.cv_results_)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can order the different models based on their performance\n",
    "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[['rank_test_score', 'param_graboost__l2_regularization', 'param_graboost__learning_rate',\n",
    "         'param_graboost__max_bins', 'param_graboost__max_depth', 'param_graboost__max_leaf_nodes',\n",
    "         'param_graboost__min_samples_leaf', 'mean_test_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HGBM Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbcm = confusion_matrix(y_test, hgbc_pred)\n",
    "hgbcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, hgbc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=hgbcpipeline, X=X_test, y=y_test, ax=ax, display_labels=hgbcpipeline.classes_)\n",
    "ax.set_title('Confusion matrix of the classifier', size=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=hgbcpipeline, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a feature importance graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permimpt = permutation_importance(estimator=hgbcpipeline, X=X_train, y=y_train, scoring=\"roc_auc\", n_repeats=15,\n",
    "                       n_jobs=-1, random_state=0)\n",
    "\n",
    "permimpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_importances = pd.Series(data=permimpt[\"importances_mean\"], index=X.columns)\n",
    "hgbc_sorted = hgbc_importances.sort_values(ascending=False)\n",
    "\n",
    "hgbc_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.barplot(y=hgbc_sorted.index, x=hgbc_sorted.values, orient=\"h\")\n",
    "\n",
    "ax.set_title(\"Hist Gradient Boosting Features Importance\", size=20)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = permutation_importance(estimator=rfpipeline, X=X_test, y=y_test, n_jobs=-1, \n",
    "                            scoring=\"roc_auc\", random_state=0, n_repeats=10)\n",
    "\n",
    "pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm2 = permutation_importance(estimator=hgbc_randm.best_estimator_, X=X_test, y=y_test, n_jobs=-1, \n",
    "                            scoring=\"roc_auc\", random_state=0, n_repeats=10)\n",
    "\n",
    "pm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = pm2.importances_mean.argsort()\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(sorted_idx)), pm2.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "plt.title('Random Forest Permutation Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (Scikit-Learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcpipeline = Pipeline(steps=[\n",
    "                        (\"preprocessor\", preprocessor),\n",
    "                        (\"xgbc\", XGBClassifier(random_state=0, \n",
    "                                               booster = 'gbtree',\n",
    "                                               objective='binary:logistic', \n",
    "                                               eval_metric=\"auc\",\n",
    "                                               scale_pos_weight=4.54,\n",
    "                                               tree_method = \"hist\"))\n",
    "                    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcpipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcpred = xgbcpipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"XGB Classifier\\n\")\n",
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, xgbcpred))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, xgbcpred))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, xgbcpred))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, xgbcpred))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, xgbcpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcm = confusion_matrix(y_test, xgbcpred)\n",
    "xgbcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, xgbcpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=xgbcpipeline, X=X_test, y=y_test, ax=ax, display_labels=xgbcpipeline.classes_)\n",
    "ax.set_title('Confusion matrix of the classifier', size=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=xgbcpipeline, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcv = cross_validate(estimator=xgbcpipeline, X=X_train, y=y_train, scoring=scoring, cv=skf, n_jobs=2)\n",
    "xgbcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcv[\"test_roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcpipeline.named_steps.xgbc.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"xgbc__n_estimators\": stats.randint(50,350),\n",
    "              \"xgbc__max_depth\" : stats.randint(3,10),\n",
    "              \"xgbc__min_child_weight\": stats.randint(0,5),\n",
    "              \"xgbc__colsample_bytree\" : stats.uniform(0,1),\n",
    "              \"xgbc__subsample\" : stats.uniform(0,1),\n",
    "              \"xgbc__eta\" : stats.uniform(0,1),\n",
    "              \"xgbc__gamma\" : stats.randint(0,10),\n",
    "              \"xgbc__reg_alpha\": stats.randint(0,10),\n",
    "              \"xgbc__reg_lambda\": stats.randint(0,10),\n",
    "              \"xgbc__max_bin\": stats.randint(0,256),              \n",
    "              \"xgbc__max_delta_step\" : stats.randint(1,10),\n",
    "              \"xgbc__scale_pos_weight\" : stats.uniform(1,5)\n",
    "              \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrandm = RandomizedSearchCV(estimator=xgbcpipeline, param_distributions = parameters, cv = 5, n_iter = 40, \n",
    "                           n_jobs=2, scoring=scoring, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrandm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrandm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestxgb = xgbrandm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrandm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrandm.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned XGB Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunedxgbcpred = bestxgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunedxgbcpred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"XGB Classifier\\n\")\n",
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, tunedxgbcpred))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, tunedxgbcpred))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, tunedxgbcpred))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, tunedxgbcpred))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, tunedxgbcpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunedxgbcm = confusion_matrix(y_test, tunedxgbcpred)\n",
    "tunedxgbcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, tunedxgbcpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=bestxgb, X=X_test, y=y_test, ax=ax, display_labels=bestxgb.classes_)\n",
    "ax.set_title('Confusion matrix of the classifier', size=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=bestxgb, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Feature importance\n",
    "\n",
    "The XGBoost library has a function called `plot_importance`, which we imported at the beginning of this notebook. This let's us check the features selected by the model as the most predictive. We can create a plot by calling this function and passing to it the best estimator from our grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "plot_importance(xgbcpipeline.named_steps.xgbc, ax=ax, title=\"XGB Classifier Feature Importance\",\n",
    "                importance_type = 'weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "plot_importance(bestxgb, ax=ax, title=\"XGB Classifier Feature Importance\",\n",
    "                importance_type = 'weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "plot_importance(xgbcpipeline.named_steps.xgbc, ax=ax, title=\"XGB Classifier Feature Importance\",\n",
    "                importance_type = 'gain')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcpipeline.named_steps.xgbc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcpipeline.named_steps.preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(xgbcpipeline.named_steps.xgbc.feature_importances_, \n",
    "                             index=xgbcpipeline.named_steps.preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances.nlargest(10).plot(kind='barh', figsize=(10,8))\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The permutation based importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = permutation_importance(estimator=xgbcpipeline, X=X_test, y=y_test, n_jobs=-1, \n",
    "                            scoring=\"roc_auc\", random_state=0, n_repeats=30)\n",
    "\n",
    "pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = pm.importances_mean.argsort()\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(sorted_idx)), pm.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "plt.title('XGB Permutation Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probability = hgbcpipeline.predict_proba(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probability = predicted_probability.round(0)[:,1]\n",
    "predicted_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"testoriginal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions with label column into a dataframe\n",
    "prediction_df = pd.DataFrame({'CustomerID': test_df[['CustomerID']].values[:, 0],\n",
    "                             'predicted_probability': predicted_probability})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "# Writing to csv for autograding purposes\n",
    "prediction_df.to_csv(\"prediction_submission.csv\", index=False)\n",
    "submission = pd.read_csv(\"prediction_submission.csv\")\n",
    "\n",
    "assert isinstance(submission, pd.DataFrame), 'You should have a dataframe named prediction_df.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "assert submission.columns[0] == 'CustomerID', 'The first column name should be CustomerID.'\n",
    "assert submission.columns[1] == 'predicted_probability', 'The second column name should be predicted_probability.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "assert submission.shape[0] == 104480, 'The dataframe prediction_df should have 104480 rows.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "assert submission.shape[1] == 2, 'The dataframe prediction_df should have 2 columns.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods (Other Methods)\n",
    "\n",
    "## Univariate Performance with Feature-engine\n",
    "\n",
    "This procedure works as follows:\n",
    "\n",
    "- Train a ML model per every single feature\n",
    "- Determine the performance of the models\n",
    "- Select features if model performance is above a certain threshold\n",
    "\n",
    "The C value in Logistic Regression is an user adjustable parameter that controls regularisation. In simple terms, higher values of C will instruct our model to fit the training set as best as possible, while lower C values will favour a simple models with coefficients closer to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbft = XGBClassifier(random_state=0, booster = 'gbtree', objective='binary:logistic', eval_metric=\"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the selector\n",
    "sel = SelectBySingleFeaturePerformance(\n",
    "    variables=None,\n",
    "    estimator=xgbft,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=5,\n",
    "    threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find predictive features\n",
    "sel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the transformer stores a dictionary of feature:metric pairs\n",
    "# notice that the roc can be positive or negative.\n",
    "# the selector selects based on the absolute value\n",
    "\n",
    "#In general, an AUC of 0.5 suggests no discrimination \n",
    "#(i.e., ability to diagnose patients with and without the disease or condition based on the test), \n",
    "#0.7 to 0.8 is considered acceptable, 0.8 to 0.9 is considered excellent, and more than 0.9 is considered outstanding\n",
    "\n",
    "sel.feature_performance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sel.feature_performance_).sort_values(ascending=False).plot.barh(figsize=(20, 8))\n",
    "plt.title('Performance of ML models trained with individual features', size=20)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('ROC Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features that will be removed\n",
    "\n",
    "sel.features_to_drop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features in the dataframes\n",
    "\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step forward feature selection\n",
    "\n",
    "Step forward feature selection starts by training a machine learning model for each feature in the dataset and selecting, as the starting feature, the one that returns the best performing model according to the evaluation criteria we choose.\n",
    "\n",
    "In the second step, it creates machine learning models for all combinations of the feature selected in the previous step and a second feature. It selects the pair that produces the best performing algorithm.\n",
    "\n",
    "It continues by adding 1 feature at a time to the features that were selected in previous steps, until a stopping criteria is reached.\n",
    "\n",
    "In theory, models with more features perform better. The algorithm will continue adding new features until a certain criteria is met. For example, until the model performance does not increase beyond a certain threshold. Or, as we show in this notebook, until a certain number of features are selected.\n",
    "\n",
    "The model performance metric can be the roc_auc for classification and the r squared for regression, for example, and it is determined by the user. \n",
    "\n",
    "Step forward feature selection is called a greedy procedure because it evaluates many possible single, double, triple, and so on feature combinations. Therefore, it is very computationally expensive and, sometimes, if the feature space is big enough, even unfeasible.\n",
    "\n",
    "Scikit-learn provides various stopping criteria to stop the search:\n",
    "\n",
    "* when a certain number of features is reached (like MLXtend) (arbitrary)\n",
    "* if the performance does not increase beyond a certain threshold (ideal but expensive)\n",
    "* selects half of the features (arbitrary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Forward Feature Selection Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"carpricemod.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:14]\n",
    "y = df.iloc[:,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within the SFS we indicate:\n",
    "\n",
    "# 1) the algorithm we want to create, in this case RandomForests\n",
    "# (note that I use few trees to speed things up)\n",
    "\n",
    "# 2) the stopping criteria: see sklearn documentation for more details\n",
    "\n",
    "# 3) whether to perform step forward or step backward\n",
    "\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the cross-validation\n",
    "\n",
    "# this is going to take a while, do not despair\n",
    "\n",
    "sfs = SFS(estimator=LinearRegression(), \n",
    "          n_features_to_select=6,\n",
    "          direction='forward',\n",
    "          scoring='r2',\n",
    "          cv=5,\n",
    "          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.n_features_to_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Forward Feature Selection Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:8]\n",
    "y = df.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within the SFS we indicate:\n",
    "\n",
    "# 1) the algorithm we want to create, in this case RandomForests\n",
    "# (note that I use few trees to speed things up)\n",
    "\n",
    "# 2) the stopping criteria: see sklearn documentation for more details\n",
    "\n",
    "# 3) whether to perform step forward or step backward\n",
    "\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the cross-validation\n",
    "\n",
    "# this is going to take a while, do not despair\n",
    "\n",
    "sfs = SFS(estimator=LogisticRegression(random_state=0), \n",
    "          n_features_to_select=4,\n",
    "          direction='forward',\n",
    "          scoring='f1',\n",
    "          cv=5,\n",
    "          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step backward feature selection\n",
    "\n",
    "Step Backward Feature Selection starts by fitting a model using all the features in the data set and determining its performance. \n",
    "\n",
    "Then, it trains models on all possible combinations of all features, minus one, and removes the feature that returns the model with the lowest performance.\n",
    "\n",
    "In the third step, it trains models in all possible combinations of the features remaining from step 2, minus 1 feature, and removes the feature that produced the lowest performing model.\n",
    "\n",
    "The algorithm stops when a certain criteria determined by the user is met. This criteria could be that the model performance does not decrease beyond a certain threshold, or alternatively, as we show in this notebook, when we reach a certain number of selected features.\n",
    "\n",
    "The evaluation metric can be the roc_auc for classification or the r squared for regression, for example, and is determined by the user.\n",
    "\n",
    "Step Backward Feature Selection is called greedy because it evaluates all possible n, and then n-1 and n-2 and so on feature combinations. Therefore, it is very computationally expensive and sometimes, if the feature space is big enough, even unfeasible.\n",
    "\n",
    "Scikit-learn provides various stopping criteria to stop the search:\n",
    "\n",
    "* when a certain number of features is reached (like MLXtend) (arbitrary)\n",
    "* if the performance does not increase beyond a certain threshold (ideal but expensive)\n",
    "* selects half of the features (arbitrary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Forward Feature Selection Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:14]\n",
    "y = df.iloc[:,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within the SFS we indicate:\n",
    "\n",
    "# 1) the algorithm we want to create, in this case RandomForests\n",
    "# (note that I use few trees to speed things up)\n",
    "\n",
    "# 2) the stopping criteria: see sklearn documentation for more details\n",
    "\n",
    "# 3) whether to perform step forward or step backward\n",
    "\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the cross-validation\n",
    "\n",
    "# this is going to take a while, do not despair\n",
    "\n",
    "sfs = SFS(estimator=LinearRegression(), \n",
    "          n_features_to_select=6,\n",
    "          direction='backward',\n",
    "          scoring='r2',\n",
    "          cv=5,\n",
    "          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.n_features_to_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Backward Feature Selection Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:8]\n",
    "y = df.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within the SFS we indicate:\n",
    "\n",
    "# 1) the algorithm we want to create, in this case RandomForests\n",
    "# (note that I use few trees to speed things up)\n",
    "\n",
    "# 2) the stopping criteria: see sklearn documentation for more details\n",
    "\n",
    "# 3) whether to perform step forward or step backward\n",
    "\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the cross-validation\n",
    "\n",
    "# this is going to take a while, do not despair\n",
    "\n",
    "sfs = SFS(estimator=LogisticRegression(random_state=0), \n",
    "          n_features_to_select=4,\n",
    "          direction='backward',\n",
    "          scoring='f1',\n",
    "          cv=5,\n",
    "          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle  \n",
    "\n",
    "When models take a long time to fit, you dont want to have to fit them more than once. If your kernel disconnects or you shut down the notebook and lose the cells output, youll have to refit the model, which can be frustrating and time-consuming. \n",
    "\n",
    "`pickle` is a tool that saves the fit model object to a specified location, then quickly reads it back in. It also allows you to use models that were fit somewhere else, without having to train them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model\n",
    "This step will ***W***rite (i.e., save) the model, in ***B***inary (hence, `wb`), to the folder designated by the above path. In this case, the name of the file we're writing is `rf_cv_model.pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.sav'\n",
    "dump(xgbnew,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we save the model, we'll never have to re-fit it when we run this notebook. Ideally, we could open the notebook, select \"Run all,\" and the cells would run successfully all the way to the end without any model retraining. \n",
    "\n",
    "For this to happen, we'll need to return to the cell where we defined our grid search and comment out the line where we fit the model. Otherwise, when we re-run the notebook, it would refit the model. \n",
    "\n",
    "Similarly, we'll also need to go back to where we saved the model as a pickle and comment out those lines.  \n",
    "\n",
    "Next, we'll add a new cell that reads in the saved model from the folder we already specified. For this, we'll use `rb` (read binary) and be sure to assign the model to the same variable name as we used above, `rf_cv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python code done by Dennis Lam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
